
TODO:
- limpiar plan y policy... 
- hacer sampling exp rtg... ✓
- train repaint sampĺing... ✓
- test repaint sampling...
- test model...
- do conditioned model on returns and test...

- agregar rendering... 
- parallelize sampling... 
- add cross attention... 
- fix wandb
- fix diffusion models...

- ver para pasar multiples task y que dataset tenga bien repartidos los task... en metaworld 

stuff to solve: 

test normalization, unnormalization, masking... 


- how to generalize the scripts between models
 - generalize configs, solve the trainer... 
 - generalize the plan, the buffer collector and the interaction with the env...  
- fix device in diffusion models... 

mode cond diff: pasa un batch con dos modos (32+32 ej) policy recibe historia..., crea el mode batch y lo pasa al modelo, el modelo crea la mascara... y despues en policy se obtienen resultados...


repaint sampling... igual que mode cond diff se hacen dos batches uno para hacer inpainting del task y otro de la acción. o se puede pasar la historia y hacer inpainting sobre el futuro.

#posible error en masking del task inference.. no se usa el ultimo estado creo... importante: se deberia maskear solo hasta el ultimo estado y dejar la ultima accion y reward unkown ... 